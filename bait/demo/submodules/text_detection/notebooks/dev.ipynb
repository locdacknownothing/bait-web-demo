{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process test ground truth for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, listdir, rename\n",
    "# import subprocess\n",
    " \n",
    "def rename_gt_1(gt_path):\n",
    "    for file_name in listdir(gt_path):\n",
    "        if \"gt_\" not in file_name:\n",
    "            file_path = path.join(gt_path, file_name)\n",
    "            new_file_name = \"gt_\" + file_name\n",
    "            new_file_path = path.join(gt_path, new_file_name)\n",
    "            rename(file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_gt_2(gt_path):\n",
    "    for file_name in listdir(gt_path):\n",
    "        file_path = path.join(gt_path, file_name)\n",
    "        file_name = file_name.split(\"_\")[-1]\n",
    "\n",
    "        zeros = \"0\" * (8 - len(file_name))\n",
    "\n",
    "        new_file_name = \"gt_im\" + zeros + file_name\n",
    "        new_file_path = path.join(gt_path, new_file_name)\n",
    "        rename(file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_gt_3(gt_path):\n",
    "    for file_name in listdir(gt_path):\n",
    "        file_path = path.join(gt_path, file_name)\n",
    "        if \"gt_\" in file_name:\n",
    "            new_file_name = file_name[3:]\n",
    "        else:\n",
    "            new_file_name = file_name\n",
    "            \n",
    "        new_file_path = path.join(gt_path, new_file_name)\n",
    "        rename(file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt_path = \"/mnt/data/src/EAST/data/test/test_gt\"\n",
    "rename_gt_3(train_gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt_path = \"/mnt/data/src/text_detection/data_cook/train_gt\"\n",
    "test_gt_path = \"/mnt/data/src/text_detection/data_cook/test_gt\"\n",
    "\n",
    "rename_gt_1(train_gt_path)\n",
    "rename_gt_1(test_gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4725, 4725)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "img_path = \"/mnt/data/data/Data_Ve_Chu/Data_Ve_Chu_copy/train_img\"\n",
    "gt_path = \"/mnt/data/data/Data_Ve_Chu/Data_Ve_Chu_copy/train_gt\"\n",
    "\n",
    "imgs = [x.split(\".\")[0] for x in os.listdir(img_path)]\n",
    "gts = [x.split(\".\")[0] for x in os.listdir(gt_path)]\n",
    "\n",
    "len(imgs), len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_path = \"/mnt/data/data/Data_Ve_Chu/Data_Ve_Chu_copy/test_img\"\n",
    "test_gt_path = \"/mnt/data/data/Data_Ve_Chu/Data_Ve_Chu_copy/test_gt\"\n",
    "\n",
    "test_imgs = [x.split(\".\")[0] for x in os.listdir(test_img_path)]\n",
    "test_gts = [x.split(\".\")[0] for x in os.listdir(test_gt_path)]\n",
    "\n",
    "len(test_imgs), len(test_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1698549704027_4',\n",
       " '1699678113962_3_POI',\n",
       " '1700297942343_1_PA',\n",
       " '1720590006319_0',\n",
       " '17211947176782',\n",
       " '17212823922968']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redundant_imgs = [x for x in imgs if x not in gts]\n",
    "redundant_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_from_id(id: str, folder_path: str) -> str:\n",
    "    file_name = id + \".jpg\"\n",
    "    return os.path.join(folder_path, file_name)\n",
    "\n",
    "def remove_images(image_paths: list[str]) -> None:\n",
    "    for image_path in image_paths:\n",
    "        os.remove(image_path)\n",
    "    \n",
    "    print(\"Removed {} redundant images\".format(len(image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 6 redundant images\n"
     ]
    }
   ],
   "source": [
    "redundant_img_paths = [get_path_from_id(x, img_path) for x in redundant_imgs]\n",
    "remove_images(redundant_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "\n",
    "def move_images_and_gts(image_folder, text_folder, dest_image_folder, dest_text_folder, num_to_move):\n",
    "    os.makedirs(dest_image_folder, exist_ok=True)\n",
    "    os.makedirs(dest_text_folder, exist_ok=True)\n",
    "\n",
    "    # List all image and text files\n",
    "    image_files = os.listdir(image_folder)\n",
    "    text_files = os.listdir(text_folder)\n",
    "\n",
    "    # Pair files by their base name (without extensions)\n",
    "    file_pairs = [(img, txt) for img in image_files for txt in text_files if os.path.splitext(img)[0] == os.path.splitext(txt)[0]]\n",
    "\n",
    "    # Shuffle and select a subset (e.g., 1000 pairs)\n",
    "    random.shuffle(file_pairs)\n",
    "    selected_pairs = file_pairs[:num_to_move]\n",
    "\n",
    "    # Move the selected pairs to the destination folders\n",
    "    for img, txt in selected_pairs:\n",
    "        shutil.move(os.path.join(image_folder, img), os.path.join(dest_image_folder, img))\n",
    "        shutil.move(os.path.join(text_folder, txt), os.path.join(dest_text_folder, txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = \"/mnt/data/data/Data_Ve_Chu/Data_Ve_Chu_copy/test_img\"\n",
    "test_gt_path = \"/mnt/data/data/Data_Ve_Chu/Data_Ve_Chu_copy/test_gt\"\n",
    "\n",
    "move_images_and_gts(img_path, gt_path, test_img_path, test_gt_path, 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "def cal_distance(x1, y1, x2, y2):\n",
    "    \"\"\"calculate the Euclidean distance\"\"\"\n",
    "    return math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
    "\n",
    "\n",
    "def move_points(vertices, index1, index2, r, coef):\n",
    "    \"\"\"move the two points to shrink edge\n",
    "    Input:\n",
    "            vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "            index1  : offset of point1\n",
    "            index2  : offset of point2\n",
    "            r       : [r1, r2, r3, r4] in paper\n",
    "            coef    : shrink ratio in paper\n",
    "    Output:\n",
    "            vertices: vertices where one edge has been shinked\n",
    "    \"\"\"\n",
    "    index1 = index1 % 4\n",
    "    index2 = index2 % 4\n",
    "    x1_index = index1 * 2 + 0\n",
    "    y1_index = index1 * 2 + 1\n",
    "    x2_index = index2 * 2 + 0\n",
    "    y2_index = index2 * 2 + 1\n",
    "\n",
    "    r1 = r[index1]\n",
    "    r2 = r[index2]\n",
    "    length_x = vertices[x1_index] - vertices[x2_index]\n",
    "    length_y = vertices[y1_index] - vertices[y2_index]\n",
    "    length = cal_distance(\n",
    "        vertices[x1_index], vertices[y1_index], vertices[x2_index], vertices[y2_index]\n",
    "    )\n",
    "    if length > 1:\n",
    "        ratio = (r1 * coef) / length\n",
    "        vertices[x1_index] += ratio * (-length_x)\n",
    "        vertices[y1_index] += ratio * (-length_y)\n",
    "        ratio = (r2 * coef) / length\n",
    "        vertices[x2_index] += ratio * length_x\n",
    "        vertices[y2_index] += ratio * length_y\n",
    "    return vertices\n",
    "\n",
    "\n",
    "def shrink_poly(vertices, coef=0.3):\n",
    "    \"\"\"shrink the text region\n",
    "    Input:\n",
    "            vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "            coef    : shrink ratio in paper\n",
    "    Output:\n",
    "            v       : vertices of shrinked text region <numpy.ndarray, (8,)>\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    r1 = min(cal_distance(x1, y1, x2, y2), cal_distance(x1, y1, x4, y4))\n",
    "    r2 = min(cal_distance(x2, y2, x1, y1), cal_distance(x2, y2, x3, y3))\n",
    "    r3 = min(cal_distance(x3, y3, x2, y2), cal_distance(x3, y3, x4, y4))\n",
    "    r4 = min(cal_distance(x4, y4, x1, y1), cal_distance(x4, y4, x3, y3))\n",
    "    r = [r1, r2, r3, r4]\n",
    "\n",
    "    # obtain offset to perform move_points() automatically\n",
    "    if cal_distance(x1, y1, x2, y2) + cal_distance(x3, y3, x4, y4) > cal_distance(\n",
    "        x2, y2, x3, y3\n",
    "    ) + cal_distance(x1, y1, x4, y4):\n",
    "        offset = 0  # two longer edges are (x1y1-x2y2) & (x3y3-x4y4)\n",
    "    else:\n",
    "        offset = 1  # two longer edges are (x2y2-x3y3) & (x4y4-x1y1)\n",
    "\n",
    "    v = vertices.copy()\n",
    "    v = move_points(v, 0 + offset, 1 + offset, r, coef)\n",
    "    v = move_points(v, 2 + offset, 3 + offset, r, coef)\n",
    "    v = move_points(v, 1 + offset, 2 + offset, r, coef)\n",
    "    v = move_points(v, 3 + offset, 4 + offset, r, coef)\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_rotate_mat(theta):\n",
    "    \"\"\"positive theta value means rotate clockwise\"\"\"\n",
    "    return np.array(\n",
    "        [[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]]\n",
    "    )\n",
    "\n",
    "\n",
    "def rotate_vertices(vertices, theta, anchor=None):\n",
    "    \"\"\"rotate vertices around anchor\n",
    "    Input:\n",
    "            vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "            theta   : angle in radian measure\n",
    "            anchor  : fixed position during rotation\n",
    "    Output:\n",
    "            rotated vertices <numpy.ndarray, (8,)>\n",
    "    \"\"\"\n",
    "    v = vertices.reshape((4, 2)).T\n",
    "    if anchor is None:\n",
    "        anchor = v[:, :1]\n",
    "    rotate_mat = get_rotate_mat(theta)\n",
    "    res = np.dot(rotate_mat, v - anchor)\n",
    "    return (res + anchor).T.reshape(-1)\n",
    "\n",
    "\n",
    "def get_boundary(vertices):\n",
    "    \"\"\"get the tight boundary around given vertices\n",
    "    Input:\n",
    "            vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "            the boundary\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    x_min = min(x1, x2, x3, x4)\n",
    "    x_max = max(x1, x2, x3, x4)\n",
    "    y_min = min(y1, y2, y3, y4)\n",
    "    y_max = max(y1, y2, y3, y4)\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "def cal_error(vertices):\n",
    "    \"\"\"default orientation is x1y1 : left-top, x2y2 : right-top, x3y3 : right-bot, x4y4 : left-bot\n",
    "    calculate the difference between the vertices orientation and default orientation\n",
    "    Input:\n",
    "            vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "            err     : difference measure\n",
    "    \"\"\"\n",
    "    x_min, x_max, y_min, y_max = get_boundary(vertices)\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    err = (\n",
    "        cal_distance(x1, y1, x_min, y_min)\n",
    "        + cal_distance(x2, y2, x_max, y_min)\n",
    "        + cal_distance(x3, y3, x_max, y_max)\n",
    "        + cal_distance(x4, y4, x_min, y_max)\n",
    "    )\n",
    "    return err\n",
    "\n",
    "\n",
    "def find_min_rect_angle(vertices):\n",
    "    \"\"\"find the best angle to rotate poly and obtain min rectangle\n",
    "    Input:\n",
    "            vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "            the best angle <radian measure>\n",
    "    \"\"\"\n",
    "    angle_interval = 1\n",
    "    angle_list = list(range(-90, 90, angle_interval))\n",
    "    area_list = []\n",
    "    for theta in angle_list:\n",
    "        rotated = rotate_vertices(vertices, theta / 180 * math.pi)\n",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = rotated\n",
    "        temp_area = (max(x1, x2, x3, x4) - min(x1, x2, x3, x4)) * (\n",
    "            max(y1, y2, y3, y4) - min(y1, y2, y3, y4)\n",
    "        )\n",
    "        area_list.append(temp_area)\n",
    "\n",
    "    sorted_area_index = sorted(list(range(len(area_list))), key=lambda k: area_list[k])\n",
    "    min_error = float(\"inf\")\n",
    "    best_index = -1\n",
    "    rank_num = 10\n",
    "    # find the best angle with correct orientation\n",
    "    for index in sorted_area_index[:rank_num]:\n",
    "        rotated = rotate_vertices(vertices, angle_list[index] / 180 * math.pi)\n",
    "        temp_error = cal_error(rotated)\n",
    "        if temp_error < min_error:\n",
    "            min_error = temp_error\n",
    "            best_index = index\n",
    "    return angle_list[best_index] / 180 * math.pi\n",
    "\n",
    "\n",
    "def is_cross_text(start_loc, length, vertices):\n",
    "    \"\"\"check if the crop image crosses text regions\n",
    "    Input:\n",
    "            start_loc: left-top position\n",
    "            length   : length of crop image\n",
    "            vertices : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "    Output:\n",
    "            True if crop image crosses text region\n",
    "    \"\"\"\n",
    "    if vertices.size == 0:\n",
    "        return False\n",
    "    start_w, start_h = start_loc\n",
    "    a = np.array(\n",
    "        [\n",
    "            start_w,\n",
    "            start_h,\n",
    "            start_w + length,\n",
    "            start_h,\n",
    "            start_w + length,\n",
    "            start_h + length,\n",
    "            start_w,\n",
    "            start_h + length,\n",
    "        ]\n",
    "    ).reshape((4, 2))\n",
    "    p1 = Polygon(a).convex_hull\n",
    "    for vertice in vertices:\n",
    "        p2 = Polygon(vertice.reshape((4, 2))).convex_hull\n",
    "        inter = p1.intersection(p2).area\n",
    "        if 0.01 <= inter / p2.area <= 0.99:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def crop_img(img, vertices, labels, length):\n",
    "    \"\"\"crop img patches to obtain batch and augment\n",
    "    Input:\n",
    "            img         : PIL Image\n",
    "            vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "            labels      : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "            length      : length of cropped image region\n",
    "    Output:\n",
    "            region      : cropped image region\n",
    "            new_vertices: new vertices in cropped region\n",
    "    \"\"\"\n",
    "    h, w = img.height, img.width\n",
    "    # confirm the shortest side of image >= length\n",
    "    if h >= w and w < length:\n",
    "        img = img.resize((length, int(h * length / w)), Image.BILINEAR)\n",
    "    elif h < w and h < length:\n",
    "        img = img.resize((int(w * length / h), length), Image.BILINEAR)\n",
    "    ratio_w = img.width / w\n",
    "    ratio_h = img.height / h\n",
    "    assert ratio_w >= 1 and ratio_h >= 1\n",
    "\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:, [0, 2, 4, 6]] = vertices[:, [0, 2, 4, 6]] * ratio_w\n",
    "        new_vertices[:, [1, 3, 5, 7]] = vertices[:, [1, 3, 5, 7]] * ratio_h\n",
    "\n",
    "    # find random position\n",
    "    remain_h = img.height - length\n",
    "    remain_w = img.width - length\n",
    "    flag = True\n",
    "    cnt = 0\n",
    "    while flag and cnt < 1000:\n",
    "        cnt += 1\n",
    "        start_w = int(np.random.rand() * remain_w)\n",
    "        start_h = int(np.random.rand() * remain_h)\n",
    "        flag = is_cross_text([start_w, start_h], length, new_vertices[labels == 1, :])\n",
    "    box = (start_w, start_h, start_w + length, start_h + length)\n",
    "    region = img.crop(box)\n",
    "    if new_vertices.size == 0:\n",
    "        return region, new_vertices\n",
    "\n",
    "    new_vertices[:, [0, 2, 4, 6]] -= start_w\n",
    "    new_vertices[:, [1, 3, 5, 7]] -= start_h\n",
    "    return region, new_vertices\n",
    "\n",
    "\n",
    "def rotate_all_pixels(rotate_mat, anchor_x, anchor_y, length):\n",
    "    \"\"\"get rotated locations of all pixels for next stages\n",
    "    Input:\n",
    "            rotate_mat: rotatation matrix\n",
    "            anchor_x  : fixed x position\n",
    "            anchor_y  : fixed y position\n",
    "            length    : length of image\n",
    "    Output:\n",
    "            rotated_x : rotated x positions <numpy.ndarray, (length,length)>\n",
    "            rotated_y : rotated y positions <numpy.ndarray, (length,length)>\n",
    "    \"\"\"\n",
    "    x = np.arange(length)\n",
    "    y = np.arange(length)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    x_lin = x.reshape((1, x.size))\n",
    "    y_lin = y.reshape((1, x.size))\n",
    "    coord_mat = np.concatenate((x_lin, y_lin), 0)\n",
    "    rotated_coord = np.dot(\n",
    "        rotate_mat, coord_mat - np.array([[anchor_x], [anchor_y]])\n",
    "    ) + np.array([[anchor_x], [anchor_y]])\n",
    "    rotated_x = rotated_coord[0, :].reshape(x.shape)\n",
    "    rotated_y = rotated_coord[1, :].reshape(y.shape)\n",
    "    return rotated_x, rotated_y\n",
    "\n",
    "\n",
    "def adjust_height(img, vertices, ratio=0.2):\n",
    "    \"\"\"adjust height of image to aug data\n",
    "    Input:\n",
    "            img         : PIL Image\n",
    "            vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "            ratio       : height changes in [0.8, 1.2]\n",
    "    Output:\n",
    "            img         : adjusted PIL Image\n",
    "            new_vertices: adjusted vertices\n",
    "    \"\"\"\n",
    "    ratio_h = 1 + ratio * (np.random.rand() * 2 - 1)\n",
    "    old_h = img.height\n",
    "    new_h = int(np.around(old_h * ratio_h))\n",
    "    img = img.resize((img.width, new_h), Image.BILINEAR)\n",
    "\n",
    "    new_vertices = vertices.copy()\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:, [1, 3, 5, 7]] = vertices[:, [1, 3, 5, 7]] * (new_h / old_h)\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def rotate_img(img, vertices, angle_range=10):\n",
    "    \"\"\"rotate image [-10, 10] degree to aug data\n",
    "    Input:\n",
    "            img         : PIL Image\n",
    "            vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "            angle_range : rotate range\n",
    "    Output:\n",
    "            img         : rotated PIL Image\n",
    "            new_vertices: rotated vertices\n",
    "    \"\"\"\n",
    "    center_x = (img.width - 1) / 2\n",
    "    center_y = (img.height - 1) / 2\n",
    "    angle = angle_range * (np.random.rand() * 2 - 1)\n",
    "    img = img.rotate(angle, Image.BILINEAR)\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        new_vertices[i, :] = rotate_vertices(\n",
    "            vertice, -angle / 180 * math.pi, np.array([[center_x], [center_y]])\n",
    "        )\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def get_score_geo(img, vertices, labels, scale, length):\n",
    "    \"\"\"generate score gt and geometry gt\n",
    "    Input:\n",
    "            img     : PIL Image\n",
    "            vertices: vertices of text regions <numpy.ndarray, (n,8)>\n",
    "            labels  : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "            scale   : feature map / image\n",
    "            length  : image length\n",
    "    Output:\n",
    "            score gt, geo gt, ignored\n",
    "    \"\"\"\n",
    "    score_map = np.zeros(\n",
    "        (int(img.height * scale), int(img.width * scale), 1), np.float32\n",
    "    )\n",
    "    geo_map = np.zeros((int(img.height * scale), int(img.width * scale), 5), np.float32)\n",
    "    ignored_map = np.zeros(\n",
    "        (int(img.height * scale), int(img.width * scale), 1), np.float32\n",
    "    )\n",
    "\n",
    "    index = np.arange(0, length, int(1 / scale))\n",
    "    index_x, index_y = np.meshgrid(index, index)\n",
    "    ignored_polys = []\n",
    "    polys = []\n",
    "\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        if labels[i] == 0:\n",
    "            ignored_polys.append(\n",
    "                np.around(scale * vertice.reshape((4, 2))).astype(np.int32)\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        poly = np.around(scale * shrink_poly(vertice).reshape((4, 2))).astype(\n",
    "            np.int32\n",
    "        )  # scaled & shrinked\n",
    "        polys.append(poly)\n",
    "        temp_mask = np.zeros(score_map.shape[:-1], np.float32)\n",
    "        cv2.fillPoly(temp_mask, [poly], 1)\n",
    "\n",
    "        theta = find_min_rect_angle(vertice)\n",
    "        rotate_mat = get_rotate_mat(theta)\n",
    "\n",
    "        rotated_vertices = rotate_vertices(vertice, theta)\n",
    "        x_min, x_max, y_min, y_max = get_boundary(rotated_vertices)\n",
    "        rotated_x, rotated_y = rotate_all_pixels(\n",
    "            rotate_mat, vertice[0], vertice[1], length\n",
    "        )\n",
    "\n",
    "        d1 = rotated_y - y_min\n",
    "        d1[d1 < 0] = 0\n",
    "        d2 = y_max - rotated_y\n",
    "        d2[d2 < 0] = 0\n",
    "        d3 = rotated_x - x_min\n",
    "        d3[d3 < 0] = 0\n",
    "        d4 = x_max - rotated_x\n",
    "        d4[d4 < 0] = 0\n",
    "        geo_map[:, :, 0] += d1[index_y, index_x] * temp_mask\n",
    "        geo_map[:, :, 1] += d2[index_y, index_x] * temp_mask\n",
    "        geo_map[:, :, 2] += d3[index_y, index_x] * temp_mask\n",
    "        geo_map[:, :, 3] += d4[index_y, index_x] * temp_mask\n",
    "        geo_map[:, :, 4] += theta * temp_mask\n",
    "\n",
    "    cv2.fillPoly(ignored_map, ignored_polys, 1)\n",
    "    cv2.fillPoly(score_map, polys, 1)\n",
    "    return (\n",
    "        torch.Tensor(score_map).permute(2, 0, 1),\n",
    "        torch.Tensor(geo_map).permute(2, 0, 1),\n",
    "        torch.Tensor(ignored_map).permute(2, 0, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_vertices(lines):\n",
    "    \"\"\"extract vertices info from txt lines\n",
    "    Input:\n",
    "            lines   : list of string info\n",
    "    Output:\n",
    "            vertices: vertices of text regions <numpy.ndarray, (n,8)>\n",
    "            labels  : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    vertices = []\n",
    "    for line in lines:\n",
    "        vertices.append(\n",
    "            list(map(int, line.rstrip(\"\\n\").lstrip(\"\\ufeff\").split(\",\")[:8]))\n",
    "        )\n",
    "        label = 0 if \"###\" in line else 1\n",
    "        labels.append(label)\n",
    "    return np.array(vertices), np.array(labels)\n",
    "\n",
    "\n",
    "class custom_dataset(data.Dataset):\n",
    "    def __init__(self, img_path, gt_path, scale=0.25, length=512):\n",
    "        super(custom_dataset, self).__init__()\n",
    "        self.img_files = [\n",
    "            os.path.join(img_path, img_file)\n",
    "            for img_file in sorted(os.listdir(img_path))\n",
    "        ]\n",
    "        self.gt_files = [\n",
    "            os.path.join(gt_path, gt_file) for gt_file in sorted(os.listdir(gt_path))\n",
    "        ]\n",
    "\n",
    "        assert len(self.img_files) == len(self.gt_files)\n",
    "\n",
    "        self.scale = scale\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            with open(self.gt_files[index], \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            vertices, labels = extract_vertices(lines)\n",
    "\n",
    "            img = Image.open(self.img_files[index])\n",
    "            img = img.convert(\"RGB\")\n",
    "            img, vertices = adjust_height(img, vertices)\n",
    "            img, vertices = rotate_img(img, vertices)\n",
    "            img, vertices = crop_img(img, vertices, labels, self.length)\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.ColorJitter(0.5, 0.5, 0.5, 0.25),\n",
    "                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            trf_img = transform(img)\n",
    "            score_map, geo_map, ignored_map = get_score_geo(\n",
    "                img, vertices, labels, self.scale, self.length\n",
    "            )\n",
    "            return trf_img, score_map, geo_map, ignored_map\n",
    "        except:\n",
    "            raise Exception(f\"Error at {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process vinai-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def count_files(directory: str) -> int:\n",
    "    return len(os.listdir(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = count_files('/mnt/data1/src/ocr/vinai-vietnamese/labels')\n",
    "# train = count_files('/mnt/data1/src/ocr/vinai-vietnamese/train_img')\n",
    "# test = count_files('/mnt/data1/src/ocr/vinai-vietnamese/test_img')\n",
    "\n",
    "# print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split labels into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_dir = '/mnt/data1/src/ocr/vinai-vietnamese/labels'\n",
    "# train_imgs_dir = '/mnt/data1/src/ocr/vinai-vietnamese/train_img'\n",
    "# test_imgs_dir = '/mnt/data1/src/ocr/vinai-vietnamese/test_img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = os.listdir(labels_dir)\n",
    "# train_imgs = os.listdir(train_imgs_dir)\n",
    "# test_imgs = os.listdir(test_imgs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels[0], train_imgs[0], test_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_image(image_path: str) -> str:\n",
    "    name = image_path.split(\".\")[0]\n",
    "    name = name[2:]\n",
    "\n",
    "    return int(name)\n",
    "\n",
    "\n",
    "def get_id_from_label(label_path: str) -> str:\n",
    "    name = label_path.split(\".\")[0]\n",
    "    name = name.split(\"_\")[-1]\n",
    "\n",
    "    return int(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids = [get_id_from_image(train_img) for train_img in train_imgs]\n",
    "# test_ids = [get_id_from_image(test_img) for test_img in test_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids[0], test_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# train_labels_dir = '/mnt/data1/src/ocr/vinai-vietnamese/train_gt'\n",
    "# test_labels_dir = '/mnt/data1/src/ocr/vinai-vietnamese/test_gt'\n",
    "\n",
    "# for label in labels:\n",
    "#     id = get_id_from_label(label)\n",
    "#     if id in train_ids:\n",
    "#         shutil.copy(os.path.join(labels_dir, label), train_labels_dir)\n",
    "#     if id in test_ids:\n",
    "#         shutil.copy(os.path.join(labels_dir, label), test_labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = count_files(\"/mnt/data1/src/ocr/vinai-vietnamese/train_gt\")\n",
    "test = count_files(\"/mnt/data1/src/ocr/vinai-vietnamese/test_gt\")\n",
    "\n",
    "print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "cfg = [\n",
    "    64,\n",
    "    64,\n",
    "    \"M\",\n",
    "    128,\n",
    "    128,\n",
    "    \"M\",\n",
    "    256,\n",
    "    256,\n",
    "    256,\n",
    "    \"M\",\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    \"M\",\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    \"M\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class extractor(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(extractor, self).__init__()\n",
    "        vgg16_bn = VGG(make_layers(cfg, batch_norm=True))\n",
    "        if pretrained:\n",
    "            vgg16_bn.load_state_dict(torch.load(\"./pths/vgg16_bn-6c64b313.pth\"))\n",
    "        self.features = vgg16_bn.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for m in self.features:\n",
    "            x = m(x)\n",
    "            if isinstance(m, nn.MaxPool2d):\n",
    "                out.append(x)\n",
    "        return out[1:]\n",
    "\n",
    "\n",
    "class merge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(merge, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1024, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(384, 64, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(192, 32, 1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.conv6 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(32)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.conv7 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.interpolate(x[3], scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        y = torch.cat((y, x[2]), 1)\n",
    "        y = self.relu1(self.bn1(self.conv1(y)))\n",
    "        y = self.relu2(self.bn2(self.conv2(y)))\n",
    "\n",
    "        y = F.interpolate(y, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        y = torch.cat((y, x[1]), 1)\n",
    "        y = self.relu3(self.bn3(self.conv3(y)))\n",
    "        y = self.relu4(self.bn4(self.conv4(y)))\n",
    "\n",
    "        y = F.interpolate(y, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        y = torch.cat((y, x[0]), 1)\n",
    "        y = self.relu5(self.bn5(self.conv5(y)))\n",
    "        y = self.relu6(self.bn6(self.conv6(y)))\n",
    "\n",
    "        y = self.relu7(self.bn7(self.conv7(y)))\n",
    "        return y\n",
    "\n",
    "\n",
    "class output(nn.Module):\n",
    "    def __init__(self, scope=512):\n",
    "        super(output, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(32, 1, 1)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.conv2 = nn.Conv2d(32, 4, 1)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.conv3 = nn.Conv2d(32, 1, 1)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        self.scope = 512\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        score = self.sigmoid1(self.conv1(x))\n",
    "        loc = self.sigmoid2(self.conv2(x)) * self.scope\n",
    "        angle = (self.sigmoid3(self.conv3(x)) - 0.5) * math.pi\n",
    "        geo = torch.cat((loc, angle), 1)\n",
    "        return score, geo\n",
    "\n",
    "\n",
    "class EAST(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EAST, self).__init__()\n",
    "        self.extractor = extractor(pretrained)\n",
    "        self.merge = merge()\n",
    "        self.output = output()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(self.merge(self.extractor(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path as sys_path\n",
    "sys_path.append(\"/mnt/data/src/text_detection\")\n",
    "\n",
    "from dataset import custom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [01:07<00:00,  7.51it/s]\n"
     ]
    }
   ],
   "source": [
    "test_image_path = \"../data_cook/test_img\" \n",
    "test_gt_path = \"../data_cook/test_gt\"\n",
    "\n",
    "test_dataset = custom_dataset(test_image_path, test_gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_dice_loss(gt_score, pred_score, training_map):\n",
    "    inter = torch.sum(gt_score * pred_score * training_map)\n",
    "    union = torch.sum(gt_score * training_map) + torch.sum(pred_score * training_map) + 1e-5\n",
    "    return 1.0 - (2 * inter / union)\n",
    "\n",
    "\n",
    "def get_geo_loss(gt_geo, pred_geo):\n",
    "    d1_gt, d2_gt, d3_gt, d4_gt, angle_gt = torch.split(gt_geo, 1, 1)\n",
    "    d1_pred, d2_pred, d3_pred, d4_pred, angle_pred = torch.split(pred_geo, 1, 1)\n",
    "    area_gt = (d1_gt + d2_gt) * (d3_gt + d4_gt)\n",
    "    area_pred = (d1_pred + d2_pred) * (d3_pred + d4_pred)\n",
    "    w_union = torch.min(d3_gt, d3_pred) + torch.min(d4_gt, d4_pred)\n",
    "    h_union = torch.min(d1_gt, d1_pred) + torch.min(d2_gt, d2_pred)\n",
    "    area_intersect = w_union * h_union\n",
    "    area_union = area_gt + area_pred - area_intersect\n",
    "    iou_loss_map = -torch.log((area_intersect + 1.0) / (area_union + 1.0))\n",
    "    angle_loss_map = 1 - torch.cos(angle_pred - angle_gt)\n",
    "    return iou_loss_map, angle_loss_map\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, weight_angle=100):\n",
    "        super(Loss, self).__init__()\n",
    "        self.weight_angle = weight_angle\n",
    "\n",
    "    def forward(self, gt_score, pred_score, gt_geo, pred_geo, ignored_map):\n",
    "        if torch.sum(gt_score) < 1:\n",
    "            return torch.sum(pred_score + pred_geo) * 0\n",
    "        \n",
    "        classify_loss = get_dice_loss(gt_score, pred_score, 1 - ignored_map)\n",
    "        iou_loss_map, angle_loss_map = get_geo_loss(gt_geo, pred_geo)\n",
    "\n",
    "        # angle_loss = torch.sum(angle_loss_map * gt_score) / torch.sum(gt_score)\n",
    "        # iou_loss = torch.sum(iou_loss_map * gt_score) / torch.sum(gt_score)\n",
    "\n",
    "        geo_loss_map = self.weight_angle * angle_loss_map + iou_loss_map\n",
    "        geo_loss = torch.mean(geo_loss_map * gt_score * (1 - ignored_map))\n",
    "\n",
    "        # return classify_loss * 0.1 + geo_loss\n",
    "\n",
    "        return (\n",
    "            torch.mean(angle_loss_map).item(),\n",
    "            torch.mean(iou_loss_map).item(),\n",
    "            torch.mean(geo_loss_map).item(),\n",
    "            classify_loss.item(),\n",
    "            geo_loss.item(),\n",
    "            (classify_loss * 0.1 + geo_loss).item(),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights is already located. No downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/src/text_detection/model.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vgg16_bn.load_state_dict(torch.load(weight_file_path))\n"
     ]
    }
   ],
   "source": [
    "from model import EAST\n",
    "from loss import Loss as Loss_\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = EAST()\n",
    "model.to(device)\n",
    "\n",
    "criterion = Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142592/1399806317.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_file = \"/mnt/data/src/text_detection/checkpoint/Data_Cook/200.pth\"\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.010089905001223087, 10.418219566345215, 11.427209854125977, 0.5983713865280151, 0.11091490834951401, 0.17075204849243164)\n",
      "(0.012468993663787842, 10.666692733764648, 11.913591384887695, 0.6158602237701416, 0.16595712304115295, 0.22754314541816711)\n",
      "(0.00887325406074524, 10.442395210266113, 11.329721450805664, 0.5900394916534424, 0.16517102718353271, 0.22417497634887695)\n",
      "(0.017246738076210022, 10.444125175476074, 12.168798446655273, 0.6000963449478149, 0.20067647099494934, 0.26068609952926636)\n",
      "(0.01353985071182251, 10.499298095703125, 11.853282928466797, 0.5772868990898132, 0.1014728844165802, 0.15920157730579376)\n",
      "(0.015570655465126038, 10.86996078491211, 12.42702579498291, 0.7287488579750061, 0.1321006566286087, 0.20497554540634155)\n",
      "(0.011238643899559975, 10.92752742767334, 12.0513916015625, 0.6355687379837036, 0.0944141149520874, 0.15797099471092224)\n",
      "(0.014813100919127464, 10.841306686401367, 12.322617530822754, 0.6675347089767456, 0.191565603017807, 0.25831907987594604)\n",
      "(0.010854551568627357, 11.01093864440918, 12.096394538879395, 0.6294991374015808, 0.10335423052310944, 0.16630414128303528)\n",
      "(0.00755518302321434, 10.167425155639648, 10.922943115234375, 0.5019892454147339, 0.16272956132888794, 0.21292848885059357)\n",
      "(0.0125764524564147, 10.399839401245117, 11.65748405456543, 0.5271917581558228, 0.14795444905757904, 0.20067362487316132)\n",
      "(0.012135202065110207, 10.467691421508789, 11.681211471557617, 0.5870116949081421, 0.2882792055606842, 0.34698036313056946)\n",
      "(0.004713243339210749, 10.327347755432129, 10.798672676086426, 0.5524202585220337, 0.10626587271690369, 0.16150790452957153)\n",
      "(0.007046506740152836, 10.483790397644043, 11.188441276550293, 0.5389196872711182, 0.05929947271943092, 0.11319144070148468)\n",
      "(0.007077815011143684, 10.738401412963867, 11.446184158325195, 0.61451655626297, 0.07206124067306519, 0.13351289927959442)\n",
      "(0.007822602055966854, 10.218961715698242, 11.001220703125, 0.5322778224945068, 0.0927291288971901, 0.14595690369606018)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for img, gt_score, gt_geo, ignored_map in test_loader:\n",
    "        img, gt_score, gt_geo, ignored_map = (\n",
    "            img.to(device),\n",
    "            gt_score.to(device),\n",
    "            gt_geo.to(device),\n",
    "            ignored_map.to(device),\n",
    "        )\n",
    "\n",
    "        pred_score, pred_geo = model(img)\n",
    "        loss = criterion(gt_score, pred_score, gt_geo, pred_geo, ignored_map)\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = \"/mnt/data1/src/ocr/vinai-vietnamese/train_img\"\n",
    "train_gt_path = \"/mnt/data1/src/ocr/vinai-vietnamese/train_gt\"\n",
    "pths_path = \"/mnt/data1/src/ocr/EAST/pths/vinai-vietnamese\"\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "num_workers = 8\n",
    "epoch_iter = 600\n",
    "save_interval = 5\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num = len(os.listdir(train_img_path))\n",
    "trainset = custom_dataset(train_img_path, train_gt_path)\n",
    "train_loader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# is_error = False\n",
    "\n",
    "# try:\n",
    "#     for img, gt_score, gt_geo, ignored_map in train_loader:\n",
    "#         continue\n",
    "# except Exception as e:\n",
    "#     is_error = True\n",
    "#     print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Loss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EAST()\n",
    "data_parallel = False\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    data_parallel = True\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[epoch_iter // 2], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(min(epoch_iter, 1000)):\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    epoch_loss = 0\n",
    "    epoch_time = time.time()\n",
    "    for i, (img, gt_score, gt_geo, ignored_map) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        img, gt_score, gt_geo, ignored_map = (\n",
    "            img.to(device),\n",
    "            gt_score.to(device),\n",
    "            gt_geo.to(device),\n",
    "            ignored_map.to(device),\n",
    "        )\n",
    "        pred_score, pred_geo = model(img)\n",
    "        loss = criterion(gt_score, pred_score, gt_geo, pred_geo, ignored_map)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(\n",
    "                \"Epoch is [{}/{}], mini-batch is [{}/{}], time consumption is {:.8f}, batch_loss is {:.8f}\".format(\n",
    "                    epoch + 1,\n",
    "                    epoch_iter,\n",
    "                    i + 1,\n",
    "                    int(file_num / batch_size),\n",
    "                    time.time() - start_time,\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        \"epoch_loss is {:.8f}, epoch_time is {:.8f}\".format(\n",
    "            epoch_loss / int(file_num / batch_size), time.time() - epoch_time\n",
    "        )\n",
    "    )\n",
    "    print(time.asctime(time.localtime(time.time())))\n",
    "    print(\"=\" * 50)\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        state_dict = model.module.state_dict() if data_parallel else model.state_dict()\n",
    "        torch.save(\n",
    "            state_dict, os.path.join(pths_path, \"model_epoch_{}.pth\".format(epoch + 1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_path = (\n",
    "    \"/mnt/data1/src/ocr/EAST/history/vinai-vietnamese/pretrained_east_vgg16_v3.pkl\"\n",
    ")\n",
    "\n",
    "with open(history_path, \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "train_loss_history = history[\"train_loss\"]\n",
    "val_loss_history = history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss history\n",
    "plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.854262077680198"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_history[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/mnt/data1/src/ocr/EAST/lanms'\n",
      "g++ -o adaptor.so -I include  -std=c++11 -O3 -I/usr/include/python3.10 -I/usr/include/python3.10  -Wno-unused-result -Wsign-compare -g      -fstack-protector-strong -Wformat -Werror=format-security  -DNDEBUG -g -fwrapv -O2 -Wall -L/usr/lib/python3.10/config-3.10-x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu  -lcrypt -ldl  -lm -lm  adaptor.cpp include/clipper/clipper.cpp --shared -fPIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "include/pybind11/common.h:491:33: warning: ‘int PyThread_create_key()’ is deprecated [-Wdeprecated-declarations]\n",
      "  491 |     decltype(PyThread_create_key()) tstate = 0; // Usually an int but a long on Cygwin64 with Python 3.x\n",
      "      |              ~~~~~~~~~~~~~~~~~~~^~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:108:36: note: declared here\n",
      "  108 | Py_DEPRECATED(3.7) PyAPI_FUNC(int) PyThread_create_key(void);\n",
      "      |                                    ^~~~~~~~~~~~~~~~~~~\n",
      "In file included from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "include/pybind11/common.h:491:33: warning: ‘int PyThread_create_key()’ is deprecated [-Wdeprecated-declarations]\n",
      "  491 |     decltype(PyThread_create_key()) tstate = 0; // Usually an int but a long on Cygwin64 with Python 3.x\n",
      "      |              ~~~~~~~~~~~~~~~~~~~^~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:108:36: note: declared here\n",
      "  108 | Py_DEPRECATED(3.7) PyAPI_FUNC(int) PyThread_create_key(void);\n",
      "      |                                    ^~~~~~~~~~~~~~~~~~~\n",
      "In file included from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "include/pybind11/cast.h: In function ‘pybind11::detail::internals& pybind11::detail::get_internals()’:\n",
      "include/pybind11/cast.h:81:31: warning: ‘void PyEval_InitThreads()’ is deprecated [-Wdeprecated-declarations]\n",
      "   81 |             PyEval_InitThreads();\n",
      "      |             ~~~~~~~~~~~~~~~~~~^~\n",
      "In file included from /usr/include/python3.10/Python.h:130,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/ceval.h:122:37: note: declared here\n",
      "  122 | Py_DEPRECATED(3.9) PyAPI_FUNC(void) PyEval_InitThreads(void);\n",
      "      |                                     ^~~~~~~~~~~~~~~~~~\n",
      "In file included from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "include/pybind11/cast.h:83:56: warning: ‘int PyThread_create_key()’ is deprecated [-Wdeprecated-declarations]\n",
      "   83 |             internals_ptr->tstate = PyThread_create_key();\n",
      "      |                                     ~~~~~~~~~~~~~~~~~~~^~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:108:36: note: declared here\n",
      "  108 | Py_DEPRECATED(3.7) PyAPI_FUNC(int) PyThread_create_key(void);\n",
      "      |                                    ^~~~~~~~~~~~~~~~~~~\n",
      "In file included from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "include/pybind11/cast.h:84:35: warning: ‘int PyThread_set_key_value(int, void*)’ is deprecated [-Wdeprecated-declarations]\n",
      "   84 |             PyThread_set_key_value(internals_ptr->tstate, tstate);\n",
      "      |             ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:110:36: note: declared here\n",
      "  110 | Py_DEPRECATED(3.7) PyAPI_FUNC(int) PyThread_set_key_value(int key,\n",
      "      |                                    ^~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from adaptor.cpp:1:\n",
      "include/pybind11/pybind11.h: In constructor ‘pybind11::gil_scoped_acquire::gil_scoped_acquire()’:\n",
      "include/pybind11/pybind11.h:1645:58: warning: ‘void* PyThread_get_key_value(int)’ is deprecated [-Wdeprecated-declarations]\n",
      " 1645 |         tstate = (PyThreadState *) PyThread_get_key_value(internals.tstate);\n",
      "      |                                    ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:112:39: note: declared here\n",
      "  112 | Py_DEPRECATED(3.7) PyAPI_FUNC(void *) PyThread_get_key_value(int key);\n",
      "      |                                       ^~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from adaptor.cpp:1:\n",
      "include/pybind11/pybind11.h:1657:35: warning: ‘int PyThread_set_key_value(int, void*)’ is deprecated [-Wdeprecated-declarations]\n",
      " 1657 |             PyThread_set_key_value(internals.tstate, tstate);\n",
      "      |             ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:110:36: note: declared here\n",
      "  110 | Py_DEPRECATED(3.7) PyAPI_FUNC(int) PyThread_set_key_value(int key,\n",
      "      |                                    ^~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from adaptor.cpp:1:\n",
      "include/pybind11/pybind11.h: In member function ‘void pybind11::gil_scoped_acquire::dec_ref()’:\n",
      "include/pybind11/pybind11.h:1696:38: warning: ‘void PyThread_delete_key_value(int)’ is deprecated [-Wdeprecated-declarations]\n",
      " 1696 |             PyThread_delete_key_value(detail::get_internals().tstate);\n",
      "      |             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:113:37: note: declared here\n",
      "  113 | Py_DEPRECATED(3.7) PyAPI_FUNC(void) PyThread_delete_key_value(int key);\n",
      "      |                                     ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from adaptor.cpp:1:\n",
      "include/pybind11/pybind11.h: In constructor ‘pybind11::gil_scoped_release::gil_scoped_release(bool)’:\n",
      "include/pybind11/pybind11.h:1724:39: warning: ‘int PyThread_set_key_value(int, void*)’ is deprecated [-Wdeprecated-declarations]\n",
      " 1724 |                 PyThread_set_key_value(key, nullptr);\n",
      "      |                 ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:110:36: note: declared here\n",
      "  110 | Py_DEPRECATED(3.7) PyAPI_FUNC(int) PyThread_set_key_value(int key,\n",
      "      |                                    ^~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from adaptor.cpp:1:\n",
      "include/pybind11/pybind11.h: In destructor ‘pybind11::gil_scoped_release::~gil_scoped_release()’:\n",
      "include/pybind11/pybind11.h:1737:35: warning: ‘int PyThread_set_key_value(int, void*)’ is deprecated [-Wdeprecated-declarations]\n",
      " 1737 |             PyThread_set_key_value(key, tstate);\n",
      "      |             ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\n",
      "In file included from /usr/include/python3.10/Python.h:122,\n",
      "                 from include/pybind11/common.h:100,\n",
      "                 from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "/usr/include/python3.10/pythread.h:110:36: note: declared here\n",
      "  110 | Py_DEPRECATED(3.7) PyAPI_FUNC(int) PyThread_set_key_value(int key,\n",
      "      |                                    ^~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from include/pybind11/pytypes.h:12,\n",
      "                 from include/pybind11/cast.h:13,\n",
      "                 from include/pybind11/attr.h:13,\n",
      "                 from include/pybind11/pybind11.h:43,\n",
      "                 from adaptor.cpp:1:\n",
      "adaptor.cpp: In function ‘PyObject* PyInit_adaptor()’:\n",
      "include/pybind11/common.h:232:33: warning: ‘PyObject* pybind11_init()’ is deprecated: PYBIND11_PLUGIN is deprecated, use PYBIND11_MODULE [-Wdeprecated-declarations]\n",
      "  232 |             return pybind11_init();                                            \\\n",
      "      |                    ~~~~~~~~~~~~~^~\n",
      "adaptor.cpp:53:1: note: in expansion of macro ‘PYBIND11_PLUGIN’\n",
      "   53 | PYBIND11_PLUGIN(adaptor) {\n",
      "      | ^~~~~~~~~~~~~~~\n",
      "include/pybind11/common.h:217:22: note: declared here\n",
      "  217 |     static PyObject *pybind11_init();                                          \\\n",
      "      |                      ^~~~~~~~~~~~~\n",
      "adaptor.cpp:53:1: note: in expansion of macro ‘PYBIND11_PLUGIN’\n",
      "   53 | PYBIND11_PLUGIN(adaptor) {\n",
      "      | ^~~~~~~~~~~~~~~\n",
      "include/clipper/clipper.cpp: In function ‘void ClipperLib::InitEdge(ClipperLib::TEdge*, ClipperLib::TEdge*, ClipperLib::TEdge*, const ClipperLib::IntPoint&)’:\n",
      "include/clipper/clipper.cpp:721:14: warning: ‘void* memset(void*, int, size_t)’ clearing an object of non-trivial type ‘struct ClipperLib::TEdge’; use assignment or value-initialization instead [-Wclass-memaccess]\n",
      "  721 |   std::memset(e, 0, sizeof(TEdge));\n",
      "      |   ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "include/clipper/clipper.cpp:66:8: note: ‘struct ClipperLib::TEdge’ declared here\n",
      "   66 | struct TEdge {\n",
      "      |        ^~~~~\n",
      "include/clipper/clipper.cpp: In member function ‘void ClipperLib::Clipper::FixupFirstLefts3(ClipperLib::OutRec*, ClipperLib::OutRec*)’:\n",
      "include/clipper/clipper.cpp:3665:13: warning: unused variable ‘firstLeft’ [-Wunused-variable]\n",
      " 3665 |     OutRec* firstLeft = ParseFirstLeft(outRec->FirstLeft);\n",
      "      |             ^~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Leaving directory '/mnt/data1/src/ocr/EAST/lanms'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from model import EAST\n",
    "import os\n",
    "from dataset import get_rotate_mat\n",
    "import numpy as np\n",
    "import lanms\n",
    "\n",
    "\n",
    "def resize_img(img):\n",
    "    \"\"\"resize image to be divisible by 32\"\"\"\n",
    "    w, h = img.size\n",
    "    resize_w = w\n",
    "    resize_h = h\n",
    "\n",
    "    resize_h = resize_h if resize_h % 32 == 0 else int(resize_h / 32) * 32\n",
    "    resize_w = resize_w if resize_w % 32 == 0 else int(resize_w / 32) * 32\n",
    "    img = img.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "    ratio_h = resize_h / h\n",
    "    ratio_w = resize_w / w\n",
    "\n",
    "    return img, ratio_h, ratio_w\n",
    "\n",
    "\n",
    "def load_pil(img):\n",
    "    \"\"\"convert PIL Image to torch.Tensor\"\"\"\n",
    "    t = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    return t(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "def is_valid_poly(res, score_shape, scale):\n",
    "    \"\"\"check if the poly in image scope\n",
    "    Input:\n",
    "            res        : restored poly in original image\n",
    "            score_shape: score map shape\n",
    "            scale      : feature map -> image\n",
    "    Output:\n",
    "            True if valid\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for i in range(res.shape[1]):\n",
    "        if (\n",
    "            res[0, i] < 0\n",
    "            or res[0, i] >= score_shape[1] * scale\n",
    "            or res[1, i] < 0\n",
    "            or res[1, i] >= score_shape[0] * scale\n",
    "        ):\n",
    "            cnt += 1\n",
    "    return True if cnt <= 1 else False\n",
    "\n",
    "\n",
    "def restore_polys(valid_pos, valid_geo, score_shape, scale=4):\n",
    "    \"\"\"restore polys from feature maps in given positions\n",
    "    Input:\n",
    "            valid_pos  : potential text positions <numpy.ndarray, (n,2)>\n",
    "            valid_geo  : geometry in valid_pos <numpy.ndarray, (5,n)>\n",
    "            score_shape: shape of score map\n",
    "            scale      : image / feature map\n",
    "    Output:\n",
    "            restored polys <numpy.ndarray, (n,8)>, index\n",
    "    \"\"\"\n",
    "    polys = []\n",
    "    index = []\n",
    "    valid_pos *= scale\n",
    "    d = valid_geo[:4, :]  # 4 x N\n",
    "    angle = valid_geo[4, :]  # N,\n",
    "\n",
    "    for i in range(valid_pos.shape[0]):\n",
    "        x = valid_pos[i, 0]\n",
    "        y = valid_pos[i, 1]\n",
    "        y_min = y - d[0, i]\n",
    "        y_max = y + d[1, i]\n",
    "        x_min = x - d[2, i]\n",
    "        x_max = x + d[3, i]\n",
    "        rotate_mat = get_rotate_mat(-angle[i])\n",
    "\n",
    "        temp_x = np.array([[x_min, x_max, x_max, x_min]]) - x\n",
    "        temp_y = np.array([[y_min, y_min, y_max, y_max]]) - y\n",
    "        coordidates = np.concatenate((temp_x, temp_y), axis=0)\n",
    "        res = np.dot(rotate_mat, coordidates)\n",
    "        res[0, :] += x\n",
    "        res[1, :] += y\n",
    "\n",
    "        if is_valid_poly(res, score_shape, scale):\n",
    "            index.append(i)\n",
    "            polys.append(\n",
    "                [\n",
    "                    res[0, 0],\n",
    "                    res[1, 0],\n",
    "                    res[0, 1],\n",
    "                    res[1, 1],\n",
    "                    res[0, 2],\n",
    "                    res[1, 2],\n",
    "                    res[0, 3],\n",
    "                    res[1, 3],\n",
    "                ]\n",
    "            )\n",
    "    return np.array(polys), index\n",
    "\n",
    "\n",
    "def get_boxes(score, geo, score_thresh=0.9, nms_thresh=0.2):\n",
    "    \"\"\"get boxes from feature map\n",
    "    Input:\n",
    "            score       : score map from model <numpy.ndarray, (1,row,col)>\n",
    "            geo         : geo map from model <numpy.ndarray, (5,row,col)>\n",
    "            score_thresh: threshold to segment score map\n",
    "            nms_thresh  : threshold in nms\n",
    "    Output:\n",
    "            boxes       : final polys <numpy.ndarray, (n,9)>\n",
    "    \"\"\"\n",
    "    score = score[0, :, :]\n",
    "    xy_text = np.argwhere(score > score_thresh)  # n x 2, format is [r, c]\n",
    "    if xy_text.size == 0:\n",
    "        return None\n",
    "\n",
    "    xy_text = xy_text[np.argsort(xy_text[:, 0])]\n",
    "    valid_pos = xy_text[:, ::-1].copy()  # n x 2, [x, y]\n",
    "    valid_geo = geo[:, xy_text[:, 0], xy_text[:, 1]]  # 5 x n\n",
    "    polys_restored, index = restore_polys(valid_pos, valid_geo, score.shape)\n",
    "    if polys_restored.size == 0:\n",
    "        return None\n",
    "\n",
    "    boxes = np.zeros((polys_restored.shape[0], 9), dtype=np.float32)\n",
    "    boxes[:, :8] = polys_restored\n",
    "    boxes[:, 8] = score[xy_text[index, 0], xy_text[index, 1]]\n",
    "    print(np.max(boxes[:, 8]))\n",
    "    boxes = lanms.merge_quadrangle_n9(boxes.astype(\"float32\"), nms_thresh)\n",
    "    print(np.max(boxes[:, 8]))\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def adjust_ratio(boxes, ratio_w, ratio_h):\n",
    "    \"\"\"refine boxes\n",
    "    Input:\n",
    "            boxes  : detected polys <numpy.ndarray, (n,9)>\n",
    "            ratio_w: ratio of width\n",
    "            ratio_h: ratio of height\n",
    "    Output:\n",
    "            refined boxes\n",
    "    \"\"\"\n",
    "    if boxes is None or boxes.size == 0:\n",
    "        return None\n",
    "    boxes[:, [0, 2, 4, 6]] /= ratio_w\n",
    "    boxes[:, [1, 3, 5, 7]] /= ratio_h\n",
    "    return np.around(boxes)\n",
    "\n",
    "\n",
    "def detect(img, model, device):\n",
    "    \"\"\"detect text regions of img using model\n",
    "    Input:\n",
    "            img   : PIL Image\n",
    "            model : detection model\n",
    "            device: gpu if gpu is available\n",
    "    Output:\n",
    "            detected polys\n",
    "    \"\"\"\n",
    "    img, ratio_h, ratio_w = resize_img(img)\n",
    "    with torch.no_grad():\n",
    "        score, geo = model(load_pil(img).to(device))\n",
    "    boxes = get_boxes(score.squeeze(0).cpu().numpy(), geo.squeeze(0).cpu().numpy())\n",
    "    return adjust_ratio(boxes, ratio_w, ratio_h)\n",
    "\n",
    "\n",
    "def plot_boxes(img, boxes):\n",
    "    \"\"\"plot boxes on image\"\"\"\n",
    "    if boxes is None:\n",
    "        return img\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for box in boxes:\n",
    "        draw.polygon(\n",
    "            [box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]],\n",
    "            outline=(0, 255, 0),\n",
    "        )\n",
    "    return img\n",
    "\n",
    "\n",
    "def detect_dataset(model, device, test_img_path, submit_path):\n",
    "    \"\"\"detection on whole dataset, save .txt results in submit_path\n",
    "    Input:\n",
    "            model        : detection model\n",
    "            device       : gpu if gpu is available\n",
    "            test_img_path: dataset path\n",
    "            submit_path  : submit result for evaluation\n",
    "    \"\"\"\n",
    "    img_files = os.listdir(test_img_path)\n",
    "    img_files = sorted(\n",
    "        [os.path.join(test_img_path, img_file) for img_file in img_files]\n",
    "    )\n",
    "\n",
    "    for i, img_file in enumerate(img_files):\n",
    "        print(\"evaluating {} image\".format(i), end=\"\\r\")\n",
    "        boxes = detect(Image.open(img_file), model, device)\n",
    "        seq = []\n",
    "        if boxes is not None:\n",
    "            seq.extend([\",\".join([str(int(b)) for b in box]) + \"\\n\" for box in boxes])\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                submit_path, \"res_\" + os.path.basename(img_file).replace(\".jpg\", \".txt\")\n",
    "            ),\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            f.writelines(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/mnt/data1/src/ocr/vinai-vietnamese/test_img/im1501.jpg\"\n",
    "# model_path = \"./pths/vinai-vietnamese/600.pth\"\n",
    "model_path = \"/mnt/data1/src/ocr/EAST/pths/east_vgg16.pth\"\n",
    "res_img = \"./res.bmp\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EAST().to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "img = Image.open(img_path)\n",
    "\n",
    "boxes = detect(img, model, device)\n",
    "# plot_img = plot_boxes(img, boxes)\n",
    "# plot_img.save(res_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
